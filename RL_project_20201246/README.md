# ê°•í™”í•™ìŠµ ê¸°ë°˜ 3ìì‚° í¬íŠ¸í´ë¦¬ì˜¤ íŠ¸ë ˆì´ë”©  
### DQN(ì´ì‚° í–‰ë™) & PPO(ì—°ì† í–‰ë™)ë¥¼ í™œìš©í•œ ë™ì  ìì‚°ë°°ë¶„ ì „ëµ êµ¬í˜„

ì´ í”„ë¡œì íŠ¸ëŠ” í•œêµ­ ì£¼ì‹ 3ì¢…ëª©(ì‚¼ì„±ì „ìÂ·í˜„ëŒ€ì°¨Â·NAVER)ì„ ëŒ€ìƒìœ¼ë¡œ  
**ê°•í™”í•™ìŠµ(Reinforcement Learning, RL)** ê¸°ë°˜ì˜ ìì‚°ë°°ë¶„ ì „ëµì„ êµ¬ì¶•í•˜ê³ ,  
ì „í†µì  ì „ëµì¸ **Equal Weight(EW)** ë° **Buy & Hold(BH)** ì™€ ë¹„êµí•˜ì—¬  
RLì´ ì´ˆê³¼ì„±ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆëŠ”ì§€ í‰ê°€í•œë‹¤.

ë³¸ ì—°êµ¬ì—ì„œëŠ”  
- **DQN(ì´ì‚°í˜• í–‰ë™ ê¸°ë°˜)**  
- **PPO(ì—°ì†í˜• í–‰ë™ ê¸°ë°˜)**  

ë‘ ê°€ì§€ RL ì•Œê³ ë¦¬ì¦˜ì„ ëª¨ë‘ ì ìš©í•˜ì˜€ìœ¼ë©°,  
ê° í™˜ê²½(Environment)ì— ë§ëŠ” state/action/rewardë¥¼ ì„¤ê³„í•˜ì—¬ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ë‹¤.

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```  
RL_project_20201246
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â””â”€â”€ processed/ â† prices.csv
â”‚
â”œâ”€â”€ notebooks/
| â”œâ”€â”€ explotarion.ipynb
â”‚ â””â”€â”€ rl_project_analysis.ipynb â† DQN + PPO í†µí•© ë¶„ì„ ë…¸íŠ¸ë¶
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ env/
â”‚ â”‚ â”œâ”€â”€ portfolio_env.py â† DQNìš© ì´ì‚°í˜• í™˜ê²½
â”‚ â”‚ â””â”€â”€ portfolio_env_continuous.py â† PPOìš© ì—°ì†í˜• í™˜ê²½
â”‚ â”‚
â”‚ â”œâ”€â”€ utils/
â”‚ â”‚ â”œâ”€â”€ baselines.py â† EW/BH baseline êµ¬í˜„ (ë¹„ìš© ë°˜ì˜)
â”‚ â”‚ â””â”€â”€ metrics.py â† ì„±ê³¼ì§€í‘œ ê³„ì‚° í•¨ìˆ˜
â”‚ â”‚
â”‚ â”œâ”€â”€ train_dqn_seeds_best.py          â† seed í•™ìŠµ + best seed ì¬í•™ìŠµ + test
â”‚ â”œâ”€â”€ train_ppo_continuous.py          â† seed í•™ìŠµ + best seed ì¬í•™ìŠµ + test
â”‚ â”œâ”€â”€ train_ppo_continuous_hparam.py   â† PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹¤í—˜ (lr/dd_penalty)
â”‚ â”œâ”€â”€ analyze_results_dqn.py
â”‚ â””â”€â”€ analyze_results_ppo.py
â”‚
â”œâ”€â”€ results/ â† DQN ê²°ê³¼ ì €ì¥
â”‚ â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ logs/
â”‚ â””â”€â”€ figures/
â”‚
â”œâ”€â”€ results_continuous/ â† PPO ê²°ê³¼ ì €ì¥
â”‚ â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ logs/
â”‚ â””â”€â”€ figures/
â”‚
â””â”€â”€ README.md

```

---

## ë°ì´í„° êµ¬ì„±

### ì‚¬ìš© ì¢…ëª©

| í‹°ì»¤ | ì¢…ëª© |
|------|------|
| SEC  | ì‚¼ì„±ì „ì |
| HYU  | í˜„ëŒ€ìë™ì°¨ |
| NAVER | NAVER |

### ê¸°ê°„ ë¶„í• 

- **Train**: ~2005â€“2016  
- **Validation**: 2017â€“2019  
- **Test**: 2020â€“2023  

ëª¨ë“  íŠ¸ë ˆì´ë”©ì€ **ì¼ ë‹¨ìœ„**ë¡œ ìˆ˜í–‰ëœë‹¤.

---

## í™˜ê²½(Environment) ì„¤ê³„

### 1) **State (ìƒíƒœ)**

#### DQN (Discrete í™˜ê²½)
- ìµœê·¼ 20ì¼ Ã— 3ìì‚° ìˆ˜ìµë¥   
- í˜„ì¬ í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘  

#### PPO (Continuous í™˜ê²½, í™•ì¥ëœ State)
- ìµœê·¼ 20ì¼ ìˆ˜ìµë¥   
- í˜„ì¬ weight  
- 20ì¼/60ì¼ ëª¨ë©˜í…€  
- 20ì¼ ë³€ë™ì„±  

---

### 2) **Action (í–‰ë™)**

#### DQN ì´ì‚° í–‰ë™
0: Hold
1: Buy SEC
2: Sell SEC
3: Buy HYU
4: Sell HYU
5: Buy NAVER
6: Sell NAVER

#### PPO ì—°ì† í–‰ë™
a âˆˆ [-1, 1]^3 â†’ weight ì‹ í˜¸

â†’ [0, max_weight] ë²”ìœ„ë¡œ ë§¤í•‘ í›„ â†’ í•©ì´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”

---

### 3) ë³´ìƒ(Reward) ì„¤ê³„

ê°•í™”í•™ìŠµ ì„±ëŠ¥ì€ ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„ì— í° ì˜í–¥ì„ ë°›ìœ¼ë¯€ë¡œ  
DQN(ì´ì‚° í–‰ë™)ê³¼ PPO(ì—°ì† í–‰ë™)ì— ì„œë¡œ ë‹¤ë¥¸ ë³´ìƒ êµ¬ì¡°ë¥¼ ì‚¬ìš©í–ˆë‹¤.

---

#### **DQN í™˜ê²½ (Discrete Action Environment)**  
DQN í™˜ê²½ì€ **ì„ í˜• ìˆœìˆ˜ìµë¥ (net return)** ê¸°ë°˜ ë³´ìƒì„ ì‚¬ìš©í•œë‹¤.

reward = net_ret - dd_penalty * drawdown - turn_penalty * turnover

- **net_ret** = í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  âˆ’ ê±°ë˜ë¹„ìš©  
- Drawdown ë° Turnover íŒ¨ë„í‹°ëŠ” ìœ„í—˜ ê³¼ë„ ë…¸ì¶œì„ ì–µì œí•˜ê¸° ìœ„í•œ ê·œì œí•­  
- NAV ì—…ë°ì´íŠ¸ ë°©ì‹  
NAV_t = NAV_{t-1} * (1 + net_ret)


ì´ êµ¬ì¡°ëŠ” ê°’ì´ ì•ˆì •ì ì´ê³  DQN í•™ìŠµì— ì í•©í•˜ë©°,
ê¸°ë³¸ì ì¸ "ìˆœì´ìµ ìµœëŒ€í™”" í˜•íƒœì˜ ë³´ìƒ ì„¤ê³„ë¥¼ ë”°ë¥¸ë‹¤.

---

#### **PPO í™˜ê²½ (Continuous Action Environment)**  
PPOëŠ” ì—°ì† í–‰ë™(action vector) ê¸°ë°˜ì´ë¯€ë¡œ  
ë³´ìƒ ë¶„í¬ë¥¼ ì•ˆì •í™”í•˜ê¸° ìœ„í•´ **ë¡œê·¸ ìˆ˜ìµë¥ (log-return)** ê¸°ë°˜ ë³´ìƒì„ ì‚¬ìš©í–ˆë‹¤.

reward = log(1 + net_ret) - dd_penalty * drawdown - turn_penalty * turnover

- ë¡œê·¸ ìˆ˜ìµë¥ ì€ ê·¹ë‹¨ê°’(outlier)ì— ëœ ë¯¼ê°  
- PPOì™€ ê°™ì€ policy gradient ê³„ì—´ ì•Œê³ ë¦¬ì¦˜ì—ì„œ í•™ìŠµ ì•ˆì •ì„±ì´ í–¥ìƒë¨  
- net_retì´ -1 ì´í•˜ë¡œ ê°€ëŠ” ë¹„ì •ìƒ ìƒí™©ì„ ë§‰ê¸° ìœ„í•´ ë‚´ë¶€ì ìœ¼ë¡œ  
  `net_ret = max(net_ret, -0.99)` í˜•íƒœì˜ ì•ˆì „ ì²˜ë¦¬ ì ìš© ê°€ëŠ¥

---

## Baseline ë¹„êµ ì „ëµ

### **Equal Weight (EW)**  
- ë§¤ì›” ë¦¬ë°¸ëŸ°ì‹±  
- RLê³¼ ë™ì¼í•œ ê±°ë˜ë¹„ìš© ì ìš© â†’ â€œê³µì •í•œ ë¹„êµâ€  

### **Buy & Hold (BH)**  
- ì´ˆê¸° ë§¤ìˆ˜ í›„ ë¦¬ë°¸ëŸ°ì‹± ì—†ìŒ  
- ê±°ë˜ë¹„ìš© ì—†ìŒ  

---

## ì‹¤í—˜ ì ˆì°¨

### 1) Train Phase
- Seeds: **0, 42, 2024**
- ê° seedë¡œ í•™ìŠµ í›„ validation ì„±ê³¼ í‰ê°€ (Sharpe ê¸°ì¤€)

### 2) Select Best Seed
- Validation Sharpeê°€ ê°€ì¥ ë†’ì€ seed ì„ íƒ

### 3) Retrain
- Train + Validation ì „ì²´ ê¸°ê°„ìœ¼ë¡œ ë‹¤ì‹œ í•™ìŠµ

### 4) Test Evaluation
- ìµœì¢… ëª¨ë¸ë¡œ Test êµ¬ê°„ NAV / ì„±ê³¼ì§€í‘œ í‰ê°€  
- Baseline(EW, BH)ê³¼ ë¹„êµ

---

# ì£¼ìš” ì‹¤í—˜ ê²°ê³¼

## **DQN (Discrete)**

| ì „ëµ | CAGR | Vol | Sharpe | MDD |
|------|--------|--------|----------|---------|
| RL_DQN | 0.0838 | 0.223 | 0.472 | -0.416 |
| EW     | 0.0900 | 0.206 | 0.521 | -0.433 |
| BH     | 0.0739 | 0.208 | 0.445 | -0.447 |

### í•´ì„
- RLì€ ìƒìŠ¹ì¥ íë¦„ì„ ì¼ì • ë¶€ë¶„ ë”°ë¼ê°€ì§€ë§Œ  
- EW ëŒ€ë¹„ **ë°˜ì‘ì´ ëŠë¦¬ê³ **, ê±°ë˜ë¹„ìš©ì´ ëˆ„ì ë˜ë©´ì„œ ì„±ê³¼ê°€ ì œí•œë¨  
- Drawdown ë°©ì–´ë„ baseline ìˆ˜ì¤€ì—ëŠ” ë¯¸ì¹˜ì§€ ëª»í•¨

---

## **PPO (Continuous)**

| ì „ëµ | CAGR | Vol | Sharpe | MDD |
|------|--------|--------|----------|---------|
| RL_PPO_CONT | 0.0593 | 0.208 | 0.380 | -0.439 |
| EW          | 0.0900 | 0.206 | 0.521 | -0.434 |
| BH          | 0.0739 | 0.208 | 0.445 | -0.447 |

### í•´ì„
- ì—°ì† í–‰ë™(Continuous action)ì€ ììœ ë„ê°€ ë§¤ìš° ë†’ì•„ PPOê°€ ì•ˆì •ì  ì •ì±…ì„ í•™ìŠµí•˜ê¸° ì–´ë ¤ì›€  
- ê°•í™”ëœ stateë¡œ ì¼ë¶€ ê°œì„ ë˜ì—ˆìœ¼ë‚˜ baseline ì´ˆê³¼ ì„±ê³¼ëŠ” ë¯¸ë‹¬  
- ê¸ˆìœµ ì‹œê³„ì—´ í™˜ê²½ì—ì„œëŠ” **Action ì„¤ê³„Â·Feature ì„¤ê³„Â·Reward ì•ˆì •ì„±**ì´ íŠ¹íˆ ì¤‘ìš”í•¨  

---

## PPO Hyperparameter ì‹¤í—˜

PPOëŠ” ì—°ì† í–‰ë™ ì •ì±…ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—  
**learning rate**ì™€ **drawdown penalty(dd_penalty)** ë³€í™”ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•œë‹¤.  
ì´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì•„ë˜ ì„¸ ê°€ì§€ ì„¤ì •ì„ ë¹„êµí–ˆë‹¤.

### ì‹¤í—˜í•œ ì„¤ì •

| Config | learning_rate | dd_penalty |
|--------|---------------|------------|
| base_lr1e-4_dd0.3  | 1e-4 | 0.3 |
| lr3e-4_dd0.3       | 3e-4 | 0.3 |
| lr1e-4_dd0.1       | 1e-4 | 0.1 |

### í•´ì„
- learning rate ì¦ê°€(lr3e-4)ëŠ” **ì„±ëŠ¥ ë¶ˆì•ˆì •**  
- dd_penalty ì™„í™”(dd=0.1)ëŠ” **ë¦¬ìŠ¤í¬ ì¦ê°€(MDD ìƒìŠ¹)**  
- ê¸°ë³¸ ì„¤ì •(base_lr1e-4_dd0.3)ì´ ê°€ì¥ ì•ˆì •ì   
- ì „ë°˜ì ìœ¼ë¡œ baseline(EW/BH)ì„ ì•ˆì •ì ìœ¼ë¡œ ì´ˆê³¼í•˜ì§€ëŠ” ëª»í•¨  
- ì‘ì€ ìì‚° ìˆ˜(3ê°œ) + ê¸ˆìœµ ì‹œê³„ì—´ ë…¸ì´ì¦ˆ í™˜ê²½ì—ì„œëŠ”  
  **reward/state êµ¬ì¡° â†’ ì„±ëŠ¥ ì˜í–¥ > hyperparameter ì˜í–¥**  
- PPOëŠ” í•™ìŠµë¥ ê³¼ dd_penalty ì¡°ì •ìœ¼ë¡œ ì •ì±… ì•ˆì •ì„± ì°¨ì´ë¥¼ ë³´ì´ë‚˜  
  ê°•í•œ ì´ˆê³¼ì„±ê³¼ëŠ” ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ

---

## í•µì‹¬ ìš”ì•½

1. RL(DQN/PPO)ì€ **ê¸°ì´ˆì ì¸ ì¶”ì„¸ ì¶”ì¢… ëŠ¥ë ¥**ì€ í•™ìŠµí–ˆì§€ë§Œ  
   baseline(EW)ì„ ì•ˆì •ì ìœ¼ë¡œ ë„˜ì–´ì„œì§€ëŠ” ëª»í–ˆë‹¤.
2. Continuous PPOëŠ” ììœ ë„ê°€ ë„ˆë¬´ í° íƒ“ì— **ì •ì±… ì•ˆì •ì„± ë¬¸ì œ** ë°œìƒ.
3. Stateì™€ Reward ì„¤ê³„ê°€ ì„±ëŠ¥ì— ê²°ì •ì  ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
4. ê¸ˆìœµ í™˜ê²½ì€ RL ì ìš© ë‚œì´ë„ê°€ ë†’ì€ ì˜ì—­ì´ë©°, **í™˜ê²½ ì„¤ê³„ê°€ ê³§ ì„±ëŠ¥**ì´ë‹¤.

---

## ì‹¤í–‰ ë°©ë²•

### DQN í•™ìŠµ + í…ŒìŠ¤íŠ¸
python src/train_dqn_discrete.py
python src/analyze_results_dqn.py


### PPO ì—°ì†ì•¡ì…˜ í•™ìŠµ + í…ŒìŠ¤íŠ¸
python src/train_ppo_continuous.py
python src/train_ppo_continuous_hparam.py
python src/analyze_results_ppo.py


### ë¶„ì„ ë…¸íŠ¸ë¶
notebooks/rl_project_analysis.ipynb

---

---

## í•™ìŠµëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (Trained Model Download)

ë³¸ í”„ë¡œì íŠ¸ì—ì„œ í•™ìŠµí•œ ëª¨ë“  ê°•í™”í•™ìŠµ ëª¨ë¸(DQN / PPO)ì€  
GitHub Releasesì— ì—…ë¡œë“œë˜ì–´ ìˆìœ¼ë©°, ì•„ë˜ ë§í¬ì—ì„œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ”— **[Download Trained Models](https://github.com/ohsy1229/RL_project/releases/latest)**

### ì œê³µë˜ëŠ” ëª¨ë¸ ëª©ë¡
- `dqn_best.zip` â€” Best DQN ëª¨ë¸  
- `dqn_seed_0.zip`, `dqn_seed_42.zip`, `dqn_seed_2024.zip`  
- `ppo_cont_best.zip` â€” Best PPO(Continuous) ëª¨ë¸  
- `ppo_cont_seed_0.zip`, `ppo_cont_seed_42.zip`, `ppo_cont_seed_2024.zip`  
- `ppo_cont_[config]_seed0.zip` â€” PPO Hyperparameter ì‹¤í—˜ ëª¨ë¸  

---




